{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习\n",
    "\n",
    "1.k近邻（kNN）\n",
    "  \n",
    "    用于：分类 from sklearn.neighbors import KNeighborsClassifier\n",
    "       \n",
    "    用于：回归 from sklearn.neighbors import KNeighborsRegressor\n",
    "            \n",
    "    主要参数：n_neighbors 设置邻居个数 邻居个数越多，决策边界越平滑，对应更简单的模型。\n",
    "  \n",
    "    总结：使用较小的邻居个数（比如3个或5个）往往可以得到比较好的结果\n",
    "        对数据进行预处理很重要\n",
    "        对于有很多特征的数据集往往效果不好\n",
    "        预测速度慢\n",
    "        实践中往往不会用到\n",
    "        \n",
    "\n",
    "2.线性模型\n",
    "\n",
    "    用于：分类 \n",
    "        \n",
    "        1.Logistic Regression from sklearn.linear_model import LogisticRegression\n",
    "       &\n",
    "        2.线性支持向量机 from sklearn.svm impoet LinearSVC\n",
    "        \n",
    "          主要参数：\n",
    "                C 决定正则化强度的权衡参数，C值越大，正则化越弱。较小的C值可以让算法尽量适应“大多数”数据点，\n",
    "                较大的C值更强调每个数据点都分类正确额的重要性。\n",
    "                \n",
    "                penalty 默认使用L2正则化，可选penalty=\"l1\"。影响正则化，影响模型使用所有特征还是特征子集。\n",
    "                \n",
    "    用于：回归\n",
    "    \n",
    "        1.线性回归（普通最小二乘法） from sklearn.linear_model import linearRegression\n",
    "        \n",
    "         主要参数：没有参数，无法控制模型的复杂度\n",
    "        \n",
    "        2.岭回归 from sklearn.linear_model import Ridge\n",
    "        \n",
    "         主要参数：alpha 默认参数alpha=1.0。L2正则化，增大alpha会使得系数更加趋向于0，从而而降低训练集性能，很可能会提高泛化性能。\n",
    "    \n",
    "        3.lasso from sklearn.linear_model import lasso\n",
    "        \n",
    "         主要参数：alpha 默认参数alpha=1.0。L1正则化，控制系数趋向于0的强度。\n",
    "                 \n",
    "               max_iter 运行迭代的最大次数。减小alpha的同时，需要增加max_iter。\n",
    "               \n",
    "        在实践中，一般首选岭回归。如果只有几个特征是重要的，选择lasso可能更好。\n",
    "        \n",
    "    总结：训练速度非常快，预测速度也很快；\n",
    "        可以推广到非常大的数据集，对稀疏数据也很有效；\n",
    "        solver=\"sag\"选项在处理大型数据时，比默认值更快；\n",
    "        斜率（权重、系数）保存在coef_属性中，偏移（截距）保存在intercept_属性中。\n",
    " \n",
    " \n",
    "3.朴素贝叶斯分类器\n",
    "   \n",
    "    用于：分类\n",
    "        GaussianNB应用于任意连续数据。\n",
    "        BernoulliNB假定输入数据为二分类数据。主要用于文本数据分类。\n",
    "        MultinomialNB假定输入数据为计数数据。主要用于文本数据分类。\n",
    "        \n",
    "    主要参数：MultinomialNB和BernoulliNB只有一个参数alpha，用于控制模型复杂度。alpha越大，模型复杂度越低。\n",
    "    \n",
    "    总结：对高维稀疏数据效果很好，对参数的鲁棒性较好。训练和预测速度快。\n",
    " \n",
    " \n",
    "4.决策树\n",
    "\n",
    "    用于：分类 from sklearn.tree import DecisionTreeClassifier\n",
    "    \n",
    "    用于：回归 from sklearn.tree import DecisionTreeRegressor\n",
    "    \n",
    "    主要参数：random_state 在内部解决平局问题\n",
    "          \n",
    "           max_depth 限制树的深度，减少过拟合\n",
    "                 \n",
    "    总结：可以利用tree模块的export_graphviz函数将树可视化。\n",
    "        feature_importances_特征重要性，介于0到1之间。\n",
    "        树的模型用于回归时，不能外推，也不能在训练数据范围之外进行预测。\n",
    "        模型容易可视化。\n",
    "        完全不依赖缩放，不需要特征预处理。\n",
    "        即使做了预剪枝，也经常会过拟合，泛化性能很差。\n",
    "\n",
    "\n",
    "5.随机森林\n",
    "\n",
    "    用于：分类 from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    用于：回归 from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    主要参数：n_estimators 构造的树的个数。总是越大越好，减低过拟合，提升鲁棒性。\n",
    "           max_features 在每个节点处，算法随机选择特征的一个子集，并对其中一个特征寻找最佳测试。控制选择的特征个数。一般使用默认值。\n",
    "           n_jobs 调节使用的内核个数。n_jobs=-1使用计算机所有内核。\n",
    "           (max_depth 预剪枝)      \n",
    "            \n",
    "    总结：作为随机森林的一部分，树被保存在estimators_属性中\n",
    "        随机森林比单独每一棵树的过拟合都要小，给出的决策边界更符合直觉。\n",
    "        不适合维度非常高的稀疏数据（比如文本数据）。\n",
    "        训练和预测的速度比线性模型慢\n",
    " \n",
    " \n",
    "6.梯度提升回归树\n",
    "\n",
    "    用于：分类 from sklearn.ensemble import GradientBoostingClassifier\n",
    "    \n",
    "    用于：回归 from sklearn.ensemble import GradientBoostingRegressor\n",
    "    \n",
    "    主要参数：max_depth 通常使用深度很小（1到5之间）的树。\n",
    "           n_estimators 集成树的数量。与随机森林不同，过大会导致过拟合。\n",
    "           learning_rate 学习率。用于控制每棵树纠正前一棵树的错误的强度。\n",
    "           \n",
    "    总结：需要仔细调参。\n",
    "        先尝试鲁棒性较好的随机森林，如果效果很好，但预测时间太长，或需要更高的精度，可以切换成梯度提升。\n",
    "        大规模问题建议xgboost包。\n",
    "\n",
    "\n",
    "7.核支持向量机（SVC）\n",
    "\n",
    "    用于：分类 from sklearn.svm import SVC\n",
    "    \n",
    "    主要参数：C 正则化参数，限制每个点的重要性（每个点的dual_coef_属性）。C值小，则单个数据点影响范围有限。\n",
    "           gamma 控制高斯核的宽度，决定了点与点之间的“靠近”是指多大距离。gamma越小，高斯核的半径越大。\n",
    "           \n",
    "    总结：线性支持向量机的扩展（向数据中心添加非线性特征，可以让线性模型变得更强大。）\n",
    "        对参数的设定和数据的缩放非常敏感。\n",
    "        要求所有特征有相似的变化范围。方法是对特征进行缩放，如MinMaxScaler。\n",
    "        如果数据量达到100000甚至更大，在运行时间和内存使用方面会可能面临挑战。\n",
    "        \n",
    "\n",
    "8.多层感知机（MLP）\n",
    "\n",
    "    用于：分类 from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    用于：回归 from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    主要参数：alpha 正则化参数，惩罚L2，默认很小0.0001（弱正则化）。\n",
    "           hidden_layer_sizes，传入一个list，控制隐层个数和各层的隐单元数量。\n",
    "           activation 激活函数。默认relu，可选tanh等。\n",
    "           max_iter 最大迭代次数，默认200。\n",
    "           solver 学习模型或算法。默认“adam”，大多数情况下很好，但对数据缩放敏感；“lbfgs”，鲁棒性好，慢；“sgd”，更高级。\n",
    "    \n",
    "    总结：要求所有输入特征变化范围相似，最理想的情况是均值为0，方差为1。\n",
    "        足够的计算时间和数据，仔细调节参数，通常优于其他算法。\n",
    "        对包含不同种类特征的数据，基于输的模型可能表现更好。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类器的不确定性估计\n",
    "\n",
    "1.决策函数 decison_function\n",
    "\n",
    "    调用：decision_function(X_test)\n",
    "    \n",
    "    返回值：为每个样本返回一个浮点数（二分类）；返回对应每个类别的“确定度分数”（多分类）\n",
    "         \n",
    "    形状：(n_samples,)（二分类）；(n_samples, n_classes)（多分类）\n",
    "    \n",
    "    取值范围：任意范围，取决于数据和模型参数\n",
    "    \n",
    "    意义：对于类别1来说，表示模型对该数据点属于“正”类的置信程度。正值表示对正类的偏好，负值表示对“反类”的偏好。（二分类）（历史原因）\n",
    "        分数较高的类别可能性更大，得分较低的类别可能性较小。\n",
    "    \n",
    "    总结：由于可以任意缩放，输出往往很难解释。\n",
    "        \n",
    "\n",
    "2.预测概率 predict_proba\n",
    "\n",
    "    调用：predict_proba(X_test)\n",
    "    \n",
    "    返回值：每个类别的概率\n",
    "    \n",
    "    形状：(n_samples, 2)（二分类）；(n_samples, n_classes)（多分类）\n",
    "    \n",
    "    取值范围：0和1之间\n",
    "    \n",
    "    意义：第一个元素是第一个类别的估计概率，第二个元素是第二个类别的估计概率。概率超过50%的类别作为模型预测结果。\n",
    "    \n",
    "    总结：比decision_function更容易理解。\n",
    "    \n",
    "        实际反映了数据依赖于模型和参数的不确定度。\n",
    "        \n",
    "        过拟合更强的模型可能做出置信程度更高的预测，即使可能是错的。\n",
    "        \n",
    "        模型给出的不确定度符合实际情况，则称为矫正模型。\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无监督学习\n",
    "\n",
    "1. 主成分分析 PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    用于：高维数据集可视化；去除噪声\n",
    "    \n",
    "    用于：特征提取\n",
    "     \n",
    "    主要参数：n_components 指定想要保留的主成分个数。默认仅旋转（并移动）数据，保留所有主成分。\n",
    "    \n",
    "           whiten 启用白化选项，将主成分缩放到相同尺度。变换结果与StandardScaler相同。\n",
    "    \n",
    "    总结：寻找旋转方向时没有用到任何类别信息，只是观察数据的相关性。\n",
    "            \n",
    "        主成分是原始特征的组合。通常不容易做出解释。\n",
    "    \n",
    "        主成分被保存在components_属性中。\n",
    "\n",
    "        测试点是主成分的加权求和。\n",
    "        \n",
    "        通过inverse_transform方法回到原始特征空间。\n",
    "\n",
    "2. 非负矩阵分解 NMF\n",
    "    from sklearn.decompostion import NMF\n",
    "    \n",
    "    用于：特征提取\n",
    "    \n",
    "    主要参数：n_components 提取分量的个数，通常小于输入特征的个数。\n",
    "    \n",
    "    总结：只能应用于每个特征都是非负的数据。\n",
    "    \n",
    "        对由多个独立源叠加而成的数据特别有用（音轨等）。\n",
    "    \n",
    "        比PCA得到的分量更容易解释（非负）。\n",
    "        \n",
    "        所有分量地位平等。\n",
    "        \n",
    "        通常不用于对数据进行编码和重建，而是在数据中寻找有趣的模式。\n",
    "        \n",
    "3.流形学习 t-SNE\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    用于：可视化\n",
    "    \n",
    "    主要参数：默认参数效果通常就很好。perplexity和early_exaggeration等可修改，但作用很小。\n",
    "    \n",
    "    总结：计算训练数据的一种新表示，但不允许变换新数据，不能用于测试集。\n",
    "    \n",
    "         对探索性数据分析有用，如果最终目标是监督学习，则很少使用。\n",
    "        \n",
    "         TSNE类没有transform方法，可以调用fit_transform方法来代替。\n",
    "         \n",
    "         t-SNE找到的二维表示，使得原始空间中距离较近的点更加靠近。\n",
    "            \n",
    "4.k均值聚类\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    用于：聚类\n",
    "    \n",
    "    用于：数据编码（将类别看做分量）\n",
    "    \n",
    "    主要参数：n_clusters 指定寻找簇的个数，默认是8。\n",
    "    \n",
    "    总结：并不存在真实的标签。数据点的簇标签保存在labels_属性中。\n",
    "            \n",
    "        可以用predict方法为新数据分配簇标签，现有模型不会改变。\n",
    "            \n",
    "        簇中心坐标保存在cluster_centers_属性中。\n",
    "          \n",
    "        k均值只能找到相对简单的形状（凸形）。\n",
    "        \n",
    "        可以用transform方法得到数据点到每个簇中心的距离。\n",
    "        \n",
    "        算法输出依赖于随机数种子。\n",
    "        \n",
    "        需要指定寻找簇的个数。\n",
    "        \n",
    "5.凝聚聚类\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    \n",
    "    用于：聚类\n",
    "    \n",
    "    主要参数：n_clusters 指定寻找簇的个数。\n",
    "            \n",
    "           linkage 规定如何度量“最相似的簇”。默认ward，使得簇中的方差增加最小；\n",
    "                                   average，所有点之间平均距离最小；\n",
    "                                   complete，簇中点之间距离最小。\n",
    "    \n",
    "    总结：凝聚算法不能对新数据做出预测，AgglomerativeClustering没有predict方法，可以调用fit_predict方法。\n",
    "        \n",
    "        无法分离像two_moons这样复杂的形状。\n",
    "    \n",
    "6.DBSCAN \n",
    "    from sklearn.cluster import DBSCAN\n",
    "    \n",
    "    用于：聚类\n",
    "    \n",
    "    主要参数：min_sample和eps 距一个给定的数据点eps的距离内至少有min_samples个数据点，则这个数据点成为核心样本（否则成为噪声），将彼此距离小于eps的核心样本放入到同一个簇中。\n",
    "    \n",
    "    总结：不需要设置簇的个数。\n",
    "    \n",
    "        可以划分具有复杂形状的簇。\n",
    "        \n",
    "        可以找出不属于任何簇的点（噪声，标记为-1）。\n",
    "        \n",
    "        不能对新数据做出预测，使用fit_predict方法。\n",
    "        \n",
    "        对数据进行缩放之后，有时会更容易找到eps的较好取值。\n",
    "        \n",
    "7.聚类算法的评估\n",
    "\n",
    "    1.调整rand指数（ARI）\n",
    "        from sklearn.metrics.cluster import adjusted_rand_score\n",
    "        \n",
    "    2.归一化互信息（NMI）\n",
    "        from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "        \n",
    "    意义：最佳值为1,0表示不相关的聚类。（ARI可以取负值）\n",
    "        \n",
    "    总结：不能用accuracy_score评估聚类，因为簇标签本身毫无意义。\n",
    "    \n",
    "        利用轮廓系数（silhouette_score）可以在没有真实值的情况下评估聚类，但实践中效果不好，不允许复杂的形状。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征工程\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
