{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习\n",
    "\n",
    "1.k近邻（kNN）\n",
    "  \n",
    "    用于：分类 from sklearn.neighbors import KNeighborsClassifier\n",
    "       \n",
    "    用于：回归 from sklearn.neighbors import KNeighborsRegressor\n",
    "            \n",
    "    主要参数：n_neighbors 设置邻居个数 邻居个数越多，决策边界越平滑，对应更简单的模型。\n",
    "  \n",
    "    总结：使用较小的邻居个数（比如3个或5个）往往可以得到比较好的结果\n",
    "        对数据进行预处理很重要\n",
    "        对于有很多特征的数据集往往效果不好\n",
    "        预测速度慢\n",
    "        实践中往往不会用到\n",
    "        \n",
    "\n",
    "2.线性模型\n",
    "\n",
    "    用于：分类 \n",
    "        \n",
    "        1.Logistic Regression from sklearn.linear_model import LogisticRegression\n",
    "       &\n",
    "        2.线性支持向量机 from sklearn.svm impoet LinearSVC\n",
    "        \n",
    "          主要参数：\n",
    "                C 决定正则化强度的权衡参数，C值越大，正则化越弱。较小的C值可以让算法尽量适应“大多数”数据点，\n",
    "                较大的C值更强调每个数据点都分类正确额的重要性。\n",
    "                \n",
    "                penalty 默认使用L2正则化，可选penalty=\"l1\"。影响正则化，影响模型使用所有特征还是特征子集。\n",
    "                \n",
    "    用于：回归\n",
    "    \n",
    "        1.线性回归（普通最小二乘法） from sklearn.linear_model import linearRegression\n",
    "        \n",
    "         主要参数：没有参数，无法控制模型的复杂度\n",
    "        \n",
    "        2.岭回归 from sklearn.linear_model import Ridge\n",
    "        \n",
    "         主要参数：alpha 默认参数alpha=1.0。L2正则化，增大alpha会使得系数更加趋向于0，从而而降低训练集性能，很可能会提高泛化性能。\n",
    "    \n",
    "        3.lasso from sklearn.linear_model import lasso\n",
    "        \n",
    "         主要参数：alpha 默认参数alpha=1.0。L1正则化，控制系数趋向于0的强度。\n",
    "                 \n",
    "               max_iter 运行迭代的最大次数。减小alpha的同时，需要增加max_iter。\n",
    "               \n",
    "        在实践中，一般首选岭回归。如果只有几个特征是重要的，选择lasso可能更好。\n",
    "        \n",
    "    总结：训练速度非常快，预测速度也很快；\n",
    "        可以推广到非常大的数据集，对稀疏数据也很有效；\n",
    "        solver=\"sag\"选项在处理大型数据时，比默认值更快；\n",
    "        斜率（权重、系数）保存在coef_属性中，偏移（截距）保存在intercept_属性中。\n",
    " \n",
    " \n",
    "3.朴素贝叶斯分类器\n",
    "   \n",
    "    用于：分类\n",
    "        GaussianNB应用于任意连续数据。\n",
    "        BernoulliNB假定输入数据为二分类数据。主要用于文本数据分类。\n",
    "        MultinomialNB假定输入数据为计数数据。主要用于文本数据分类。\n",
    "        \n",
    "    主要参数：MultinomialNB和BernoulliNB只有一个参数alpha，用于控制模型复杂度。alpha越大，模型复杂度越低。\n",
    "    \n",
    "    总结：对高维稀疏数据效果很好，对参数的鲁棒性较好。训练和预测速度快。\n",
    " \n",
    " \n",
    "4.决策树\n",
    "\n",
    "    用于：分类 from sklearn.tree import DecisionTreeClassifier\n",
    "    \n",
    "    用于：回归 from sklearn.tree import DecisionTreeRegressor\n",
    "    \n",
    "    主要参数：random_state 在内部解决平局问题\n",
    "          \n",
    "           max_depth 限制树的深度，减少过拟合\n",
    "                 \n",
    "    总结：可以利用tree模块的export_graphviz函数将树可视化。\n",
    "        feature_importances_特征重要性，介于0到1之间。\n",
    "        树的模型用于回归时，不能外推，也不能在训练数据范围之外进行预测。\n",
    "        模型容易可视化。\n",
    "        完全不依赖缩放，不需要特征预处理。\n",
    "        即使做了预剪枝，也经常会过拟合，泛化性能很差。\n",
    "\n",
    "\n",
    "5.随机森林\n",
    "\n",
    "    用于：分类 from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    用于：回归 from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    主要参数：n_estimators 构造的树的个数。总是越大越好，减低过拟合，提升鲁棒性。\n",
    "           max_features 在每个节点处，算法随机选择特征的一个子集，并对其中一个特征寻找最佳测试。控制选择的特征个数。一般使用默认值。\n",
    "           n_jobs 调节使用的内核个数。n_jobs=-1使用计算机所有内核。\n",
    "           (max_depth 预剪枝)      \n",
    "            \n",
    "    总结：作为随机森林的一部分，树被保存在estimators_属性中\n",
    "        随机森林比单独每一棵树的过拟合都要小，给出的决策边界更符合直觉。\n",
    "        不适合维度非常高的稀疏数据（比如文本数据）。\n",
    "        训练和预测的速度比线性模型慢\n",
    " \n",
    " \n",
    "6.梯度提升回归树\n",
    "\n",
    "    用于：分类 from sklearn.ensemble import GradientBoostingClassifier\n",
    "    \n",
    "    用于：回归 from sklearn.ensemble import GradientBoostingRegressor\n",
    "    \n",
    "    主要参数：max_depth 通常使用深度很小（1到5之间）的树。\n",
    "           n_estimators 集成树的数量。与随机森林不同，过大会导致过拟合。\n",
    "           learning_rate 学习率。用于控制每棵树纠正前一棵树的错误的强度。\n",
    "           \n",
    "    总结：需要仔细调参。\n",
    "        先尝试鲁棒性较好的随机森林，如果效果很好，但预测时间太长，或需要更高的精度，可以切换成梯度提升。\n",
    "        大规模问题建议xgboost包。\n",
    "\n",
    "\n",
    "7.核支持向量机\n",
    "\n",
    "    用于：分类 from sklearn.svm import SVC\n",
    "    \n",
    "    主要参数：C 正则化参数，限制每个点的重要性（每个点的dual_coef_属性）。C值小，则单个数据点影响范围有限。\n",
    "           gamma 控制高斯核的宽度，决定了点与点之间的“靠近”是指多大距离。gamma越小，高斯核的半径越大。\n",
    "           \n",
    "    总结：线性支持向量机的扩展（向数据中心添加非线性特征，可以让线性模型变得更强大。）\n",
    "        对参数的设定和数据的缩放非常敏感。\n",
    "        要求所有特征有相似的变化范围。方法是对特征进行缩放，如MinMaxScaler。\n",
    "        如果数据量达到100000甚至更大，在运行时间和内存使用方面会可能面临挑战。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
